categories:

  - data-filter: Equivariance
    category-name: Equivariance

  - data-filter: Neural Operator
    category-name: Neural Operator

  - data-filter: Bioinformatics
    category-name: Bioinformatics


projects:

  - title: "CLIPSym: Delving into Symmetry Detection with CLIP"
    system-name:
    gif: assets/img/paper_fig/clip_sym.png
    conference: ICCV 2025 
    conference-web:
    status:
    authors: Tinghan Yang, <u>Md Ashiqur Rahman</u>, Raymond A. Yeh
    pdf: https://arxiv.org/pdf/2508.14197
    code: https://github.com/timyoung2333/CLIPSym
    website: 
    demo: 
    slides:
    talk:
    tutorial: 
    abstract-less: Symmetry is one of the most fundamental geometric cues in computer vision, and detecting it has been an ongoing challenge. 
    abstract-more: With the recent advances in vision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP model can aid symmetry detection by leveraging the additional symmetry cues found in the natural image descriptions. We propose CLIPSym, which leverages CLIP's image and language encoders and a rotation-equivariant decoder based on a hybrid of Transformer and G-Convolution to detect rotation and reflection symmetries. To fully utilize CLIP's language encoder, we have developed a novel prompting technique called Semantic-Aware Prompt Grouping (SAPG), which aggregates a diverse set of frequent object-based prompts to better integrate the semantic cues for symmetry detection. Empirically, we show that CLIPSym outperforms the current state-of-the-art on three standard symmetry detection datasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations verifying the benefits of CLIP's pre-training, the proposed equivariant decoder, and the SAPG technique. 
    tag: more_equivariance
    category: Equivariance

  - title: Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer
    system-name:
    gif: assets/img/paper_fig/local_scale_logo.png
    conference: ICCV 2025 
    conference-web:
    status:
    authors: <u>Md Ashiqur Rahman</u>, Chiao-An Yang, Michael N. Cheng, Lim Jun Hao, Jeremiah Jiang, Teck-Yian Lim, Raymond A. Yeh
    pdf: https://arxiv.org/pdf/2508.14187
    code: https://github.com/ashiq24/local-scale-equivariance
    website: https://ashiq24.github.io/local-scale-equivariance/
    demo: 
    slides:
    talk:
    tutorial: 
    abstract-less: Scale variation is a fundamental challenge in computer vision. Objects of the same class can have different sizes 
    abstract-more: and their perceived size is further affected by the distance from the camera. These variations are local to the objects, i.e., different object sizes may change differently within the same image. To effectively handle scale variations, we present a deep equilibrium canonicalizer (DEC) to improve the local scale equivariance of a model. DEC can be easily incorporated into existing network architectures and can be adapted to a pre-trained model. Notably, we show that on the competitive ImageNet benchmark, DEC improves both model performance and local scale consistency across four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. 
    tag: more_equivariance
    category: Equivariance

  - title: Group Downsampling with Equivariant Anti-aliasing
    system-name:
    gif: assets/img/paper_fig/group_dwn.jpg
    conference: ICLR 2025 
    conference-web:
    status:
    authors: <u>Md Ashiqur Rahman</u>, and Raymond A. Yeh.
    pdf: https://openreview.net/pdf?id=sOte83GogU
    code: https://github.com/ashiq24/Group_Sampling
    demo: https://colab.research.google.com/drive/1Fkuid94wnVltsgwOblvbuGaVMjLl1YCr?usp=sharing
    slides: 
    talk:
    tutorial: https://huggingface.co/blog/ashiq24/group-sampling
    abstract-less: Downsampling layers are crucial building blocks in CNN architectures, which help to increase the receptive field for learning high-level features and reduce the amount of memory/computation in the model. 
    abstract-more: In this work, we study the generalization of the uniform downsampling layer for group equivariant architectures, e.g., $G$-CNNs. That is, we aim to downsample signals (feature maps) on general finite groups with anti-aliasing. This involves the following (a) Given a finite group and a downsampling rate, we present an algorithm to form a suitable choice of subgroup. (b) Given a group and a subgroup, we study the notion of bandlimited-ness and propose how to perform anti-aliasing. Notably, our method generalizes the notion of downsampling based on classical sampling theory. When the signal is on a cyclic group, i.e., periodic, our method recovers the standard downsampling of an ideal low-pass filter followed by a subsampling operation. Finally, we conducted experiments on image classification tasks demonstrating that the proposed downsampling operation improves accuracy, better preserves equivariance, and reduces model size when incorporated into $G$-equivariant networks.
    tag: more_equivariance
    category: Equivariance

  - title: Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs.
    system-name: 
    gif: assets/img/paper_fig/codano.png
    conference: NeurIPS 2024 
    conference-web: 
    status:
    authors: <u>Md Ashiqur Rahman</u>, Robert Joseph George, Mogab Elleithy, Daniel Leibovici, Zongyi Li, Boris Bonev, Colin White, Julius Berner, Raymond A. Yeh, Jean Kossaifi, Kamyar Azizzadenesheli, Anima Anandkumar
    pdf: https://arxiv.org/pdf/2403.12553
    code: https://github.com/neuraloperator/CoDA-NO
    demo: https://colab.research.google.com/drive/1W6Qy5Mk_vEjZgrA0tWMespXqKEYDOdc6?usp=sharing
    slides:
    poster: 
    abstract-less: Existing neural operator architectures face challenges when solving multiphysics problems with coupled partial differential equations (PDEs), due to complex geometries, 
    abstract-more: interactions between physical variables, and the lack of large amounts of high-resolution training data. To address these issues, we propose Codomain Attention Neural Operator (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems. Specifically, we extend positional encoding, self-attention, and normalization layers to the function space. CoDA-NO can learn representations of different PDE systems with a single model. We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs over multiple systems by considering few-shot learning settings. On complex downstream tasks with limited data, such as fluid flow simulations and fluid-structure interactions, we found CoDA-NO to outperform existing methods on the few-shot learning task by over 36%.
    tag: more_neural_operator
    category: Neural Operator

  - title: Truly Scale-Equivariant Deep Nets with Fourier Layers.
    system-name:
    gif: assets/img/paper_fig/scale_eq.png
    conference: NeurIPS 2023 
    conference-web:
    status:
    authors: <u>Md Ashiqur Rahman</u>, and Raymond A. Yeh.
    pdf: https://arxiv.org/pdf/2311.02922
    code: https://github.com/ashiq24/Scale_Equivarinat_Fourier_Layer/
    demo: https://colab.research.google.com/drive/1fKHxYw1QxJ1CWpDFGLdl8Im83GnfAbFC?usp=sharing
    slides: 
    talk:
    poster: https://ashiq24.github.io/scale_equivariant_fourier_layer/resource/poster_thumb.pdf
    abstract-less: In computer vision, models must be able to adapt to changes in image resolution to effectively carry out tasks such as image segmentation
    abstract-more: ; This is known as scale-equivariance. Recent works have made progress in developing scale-equivariant convolutional neural networks, e.g., through weight-sharing and kernel resizing. However, these networks are not truly scale-equivariant in practice. Specifically, they do not consider anti-aliasing as they formulate the down-scaling operation in the continuous domain. To address this shortcoming, we directly formulate down-scaling in the discrete domain with consideration of anti-aliasing. We then propose a novel architecture based on Fourier layers to achieve truly scale-equivariant deep nets, i.e., absolute zero equivariance-error. Following prior works, we test this model on MNIST-scale and STL-10 datasets. Our proposed model achieves competitive classification performance while maintaining zero equivariance-error.
    tag: more_equivariance
    category: Equivariance

  - title: 'U-NO: U-shaped Neural Operators'
    system-name: 
    gif: assets/img/paper_fig/uno.png
    conference: Transactions on Machine Learning Research 2023
    conference-web: 
    status:
    authors: <u>Md Ashiqur Rahman</u>, Zachary E. Ross, Kamyar Azizzadenesheli.
    pdf: https://openreview.net/pdf?id=j3oQF9coJd
    code: https://github.com/ashiq24/UNO
    demo: https://colab.research.google.com/drive/1f1WYsjAgIjJRFtfQYYnZCZsxl602MMPX?usp=sharing
    slides: 
    talk: 
    poster: 
    abstract-less: Neural operators generalize classical neural networks to maps between infinite-dimensional spaces, e.g., function spaces.
    abstract-more: Prior works on neural operators proposed a series of novel methods to learn such maps and demonstrated unprecedented success in learning solution operators of partial differential equations. Due to their close proximity to fully connected architectures, these models mainly suffer from high memory usage and are generally limited to shallow deep learning models. In this paper, we propose U-shaped Neural Operator (U-NO), a U-shaped memory enhanced architecture that allows for deeper neural operators. U-NOs exploit the problem structures in function predictions and demonstrate fast training, data efficiency, and robustness with respect to hyperparameters choices. We study the performance of U-NO on PDE benchmarks, namely, Darcy's flow law and the Navier-Stokes equations. We show that U-NO results in an average of 26% and 44% prediction improvement on Darcy's flow and turbulent Navier-Stokes equations, respectively, over the state of the art. On Navier-Stokes 3D spatiotemporal operator learning task, we show U-NO provides 37\% improvement over the state of art methods.
    tag: more_neural_operator
    category: Neural Operator

  - title: Generative Adversarial Neural Operators.
    system-name: 
    gif: assets/img/paper_fig/gano.jpg
    conference: Transactions on Machine Learning Research 2022
    conference-web: 
    status: 
    authors: <u>Md Ashiqur Rahman</u>, Manuel A. Florez, Anima Anandkumar, Zachary E. Ross, Kamyar Azizzadenesheli.
    pdf: https://openreview.net/pdf?id=X1VzbBU6xZ
    code: https://github.com/neuraloperator/GANO
    demo: 
    slides: 
    talk: 
    poster: 
    abstract-less: We propose the generative adversarial neural operator (GANO), a generative model paradigm for learning probabilities on infinite-dimensional function spaces.
    abstract-more: The natural sciences and engineering are known to have many types of data that are sampled from infinite-dimensional function spaces, where classical finite-dimensional deep generative adversarial networks (GANs) may not be directly applicable. GANO generalizes the GAN framework and allows for the sampling of functions by learning push-forward operator maps in infinite-dimensional spaces. GANO consists of two main components, a generator neural operator and a discriminator neural functional. The inputs to the generator are samples of functions from a user-specified probability measure, e.g., Gaussian random field (GRF), and the generator outputs are synthetic data functions. The input to the discriminator is either a real or synthetic data function. In this work, we instantiate GANO using the Wasserstein criterion and show how the Wasserstein loss can be computed in infinite-dimensional spaces. We empirically study GANO in controlled cases where both input and output functions are samples from GRFs and compare its performance to the finite-dimensional counterpart GAN. We empirically study the efficacy of GANO on real-world function data of volcanic activities and show its superior performance over GAN.
    tag: more_neural_operator
    category: Neural Operator
  
  - title: 'PaCMO: Partner Dependent Human Motion Generation in Dyadic Human Activity using Neural Operators.'
    system-name: 
    gif: assets/img/paper_fig/pacmo.jpg
    conference: ArXiv 2022
    conference-web: 
    status: 
    authors: <U>Md Ashiqur Rahman</u>, Jasorsi Ghosh, Hrishikesh Viswanath, Kamyar Azizzadenesheli, Aniket Bera.
    pdf: https://arxiv.org/pdf/2211.16210
    code: 
    demo:
    slides: 
    talk: 
    poster: 
    abstract-less: We address the problem of generating 3D human motions in dyadic activities. In contrast to the concurrent works, which mainly focus on
    abstract-more: generating the motion of a single actor from the textual description, we generate the motion of one of the actors from the motion of the other participating actor in the action. This is a particularly challenging, under-explored problem, that requires learning intricate relationships between the motion of two actors participating in an action and also identifying the action from the motion of one actor. To address these, we propose partner conditioned motion operator (PaCMO), a neural operator-based generative model which learns the distribution of human motion conditioned by the partner's motion in function spaces through adversarial training. Our model can handle long unlabeled action sequences at arbitrary time resolution. We also introduce the "Functional Frechet Inception Distance" (F2ID) metric for capturing similarity between real and generated data for function spaces. We test PaCMO on NTU RGB+D and DuetDance datasets and our model produces realistic results evidenced by the F2ID score and the conducted user study.
    tag: more_neural_operator
    category: Neural Operator

  - title: "BEENE: Deep Learning-based Nonlinear Embedding Improves Batch Effect Estimation.BEENE: Deep Learning-based Nonlinear Embedding Improves Batch Effect Estimation."
    system-name: 
    gif: assets/img/paper_fig/beene.jpeg
    conference: Bioinformatics, Volume 39, 2023
    conference-web: https://academic.oup.com/bioinformatics
    status: 
    authors: <u>Md Ashiqur Rahman</u>, Abdullah Aman Tutul, Mahfuza Sharmin, Md Shamsuzzoha Bayzid
    pdf: https://academic.oup.com/bioinformatics/article/39/8/btad479/7240486?login=false
    code: https://github.com/ashiq24/BEENE
    demo: 
    slides: 
    talk: 
    poster: 
    abstract-less: Analyzing large-scale single-cell transcriptomic datasets generated using different technologies is challenging due to the presence of batch-specific systematic variations known as batch effects.
    abstract-more: Since biological and technological differences are often interspersed, detecting and accounting for batch effects in RNA-seq datasets are critical for effective data integration and interpretation. Low-dimensional embeddings, such as principal component analysis (PCA) are widely used in visual inspection and estimation of batch effects. Linear dimensionality reduction methods like PCA are effective in assessing the presence of batch effects, especially when batch effects exhibit linear patterns. However, batch effects are inherently complex and existing linear dimensionality reduction methods could be inadequate and imprecise in the presence of sophisticated nonlinear batch effects. Results:- We present Batch Effect Estimation using Nonlinear Embedding (BEENE), a deep nonlinear auto-encoder network which is specially tailored to generate an alternative lower dimensional embedding suitable for both linear and nonlinear batch effects. BEENE simultaneously learns the batch and biological variables from RNA-seq data, resulting in an embedding that is more robust and sensitive than PCA embedding in terms of detecting and quantifying batch effects. BEENE was assessed on a collection of carefully controlled simulated datasets as well as biological datasets, including two technical replicates of mouse embryogenesis cells, peripheral blood mononuclear cells from three largely different experiments and five studies of pancreatic islet cells
    tag: more_bioinformatics
    category: Bioinformatics

  - title: 'CHAPAO: Likelihood and hierarchical reference-based representation of biomolecular sequences and applications to compressing multiple sequence alignments.'
    system-name: 
    gif: assets/img/paper_fig/chapao.jpg
    conference: PLOS ONE 2022
    conference-web: https://journals.plos.org/plosone/
    status: 
    authors: <u>Md Ashiqur Rahman</u>, Abdullah Aman Tutul, Sifat Muhammad Abdullah, Md. Shamsuzzoha Bayzid.
    pdf: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0265360
    code: https://github.com/ashiq24/CHAPAO
    demo: 
    slides: 
    talk: 
    poster: 
    abstract-less: High-throughput experimental technologies are generating tremendous amounts of genomic data, offering valuable resources to answer important questions and extract biological insights. Storing this sheer amount of genomic data has become a major concern in bioinformatics.
    abstract-more: General purpose compression techniques (e.g. gzip, bzip2, 7-zip) are being widely used due to their pervasiveness and relatively good speed. However, they are not customized for genomic data and may fail to leverage special characteristics and redundancy of the biomolecular sequences. Results:- We present a new lossless compression method CHAPAO (COmpressing Alignments using Hierarchical and Probabilistic Approach), which is especially designed for multiple sequence alignments (MSAs) of biomolecular data and offers very good compression gain. We have introduced a novel hierarchical referencing technique to represent biomolecular sequences which combines likelihood based analyses of the sequence similarities and graph theoretic algorithms. We performed an extensive evaluation study using a collection of real biological data from the avian phylogenomics project, 1000 plants project (1KP), and 16S and 23S rRNA datasets. We report the performance of CHAPAO in comparison with general purpose compression techniques as well as with MFCompress and Nucleotide Archival Format (NAF)—two of the best known methods especially designed for FASTA files. Experimental results suggest that CHAPAO offers significant improvements in compression gain over most other alternative methods.
    tag: more_bioinformatics
    category: Bioinformatics
